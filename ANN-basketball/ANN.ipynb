{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gabriel Marcelino\n",
    "September 2024\n",
    "Artificial Neural Network (ANN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dependencies and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import random\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Input, Dropout\n",
    "from keras.regularizers import l2\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "pool = []\n",
    "training_data = []\n",
    "# create pool with players from 2019-2022\n",
    "with open('all_seasons.csv', mode = 'r') as file:\n",
    "    csvFile = csv.reader(file)\n",
    "    # ignore first line\n",
    "    next(csvFile)\n",
    "    for lines in csvFile:\n",
    "        year = int(lines[21][:4])\n",
    "        if 2010 < year < 2015 and len(pool) < 100 and lines[1] not in pool:\n",
    "            pool.append(lines)\n",
    "        elif len(training_data) < 5000:\n",
    "            training_data.append(lines)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimal Team : Considerations\n",
    "For my optimal team, I will aim for:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(pool):\n",
    "    features_list = []\n",
    "    for player in pool:\n",
    "        # extract relevant features based on considerations above\n",
    "        features = {\n",
    "            'name': player[1],\n",
    "            'ts_pct': player[19],\n",
    "            'reb': player[13],\n",
    "            'dreb_pct': player[17],\n",
    "            'rating': player[15],\n",
    "            'ast': player[14]\n",
    "        }\n",
    "        features_list.append(features)\n",
    "    return features_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate Training Data to train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_data(sample, batch_size=100):\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    # Define the importance of each attribute (weights)\n",
    "    weights = {\n",
    "        'ast': 0.2,        # Assists\n",
    "        'ts_pct': 0.3,     # True Shooting Percentage\n",
    "        'reb': 0.25,       # Rebounds\n",
    "        'dreb_pct': 0.1,   # Defensive Rebound Percentage\n",
    "        'rating': 0.15     # Net Rating\n",
    "    }\n",
    "\n",
    "    # Define indices for each attribute in the player data\n",
    "    ts_pct_idx = 18\n",
    "    reb_idx = 12\n",
    "    dreb_pct_idx = 16\n",
    "    rating_idx = 15\n",
    "    ast_idx = 13\n",
    "    for i in range(0, len(sample), batch_size):\n",
    "        batch = sample[i:i + batch_size]\n",
    "        if len(batch) < 5:\n",
    "            continue\n",
    "\n",
    "        for j in range(0, len(batch), 5):\n",
    "            team = batch[j:j + 5]\n",
    "            if len(team) < 5:\n",
    "                continue\n",
    "\n",
    "            team_features = []\n",
    "            team_rating = 0\n",
    "            for player in team:\n",
    "                try:\n",
    "                    player_features = [\n",
    "                        float(player[ts_pct_idx]) * weights['ts_pct'],\n",
    "                        float(player[reb_idx]) * weights['reb'],\n",
    "                        float(player[dreb_pct_idx]) * weights['dreb_pct'],\n",
    "                        float(player[rating_idx]) * weights['rating'],\n",
    "                        float(player[ast_idx]) * weights['ast']\n",
    "                    ]\n",
    "                    team_features.extend(player_features)\n",
    "                    team_rating += float(player[rating_idx])\n",
    "                except (IndexError, ValueError) as e:\n",
    "                    print(f\"Error processing player data: {e}\")\n",
    "\n",
    "            X.append(team_features)\n",
    "            y.append(1 if team_rating > 0 else 0)\n",
    "\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    print(f\"Shape of X: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape}\")\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model\n",
    "Now that we have the training data, we can train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (1000, 25)\n",
      "Shape of y: (1000,)\n",
      "Epoch 1/20\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.3752 - loss: 0.9628 - val_accuracy: 0.5467 - val_loss: 0.7103 - learning_rate: 0.0010\n",
      "Epoch 2/20\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.4746 - loss: 0.7724 - val_accuracy: 0.6633 - val_loss: 0.6565 - learning_rate: 0.0010\n",
      "Epoch 3/20\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 867us/step - accuracy: 0.4999 - loss: 0.7312 - val_accuracy: 0.7300 - val_loss: 0.6261 - learning_rate: 0.0010\n",
      "Epoch 4/20\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 963us/step - accuracy: 0.6002 - loss: 0.6711 - val_accuracy: 0.8167 - val_loss: 0.5963 - learning_rate: 0.0010\n",
      "Epoch 5/20\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 902us/step - accuracy: 0.6411 - loss: 0.6133 - val_accuracy: 0.8400 - val_loss: 0.5641 - learning_rate: 0.0010\n",
      "Epoch 6/20\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 808us/step - accuracy: 0.6452 - loss: 0.6260 - val_accuracy: 0.8833 - val_loss: 0.5310 - learning_rate: 0.0010\n",
      "Epoch 7/20\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 830us/step - accuracy: 0.6490 - loss: 0.6221 - val_accuracy: 0.9033 - val_loss: 0.4925 - learning_rate: 0.0010\n",
      "Epoch 8/20\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 804us/step - accuracy: 0.7016 - loss: 0.5621 - val_accuracy: 0.9200 - val_loss: 0.4517 - learning_rate: 0.0010\n",
      "Epoch 9/20\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 842us/step - accuracy: 0.7561 - loss: 0.5396 - val_accuracy: 0.9267 - val_loss: 0.4079 - learning_rate: 0.0010\n",
      "Epoch 10/20\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 845us/step - accuracy: 0.7498 - loss: 0.5379 - val_accuracy: 0.9300 - val_loss: 0.3658 - learning_rate: 0.0010\n",
      "Epoch 11/20\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 831us/step - accuracy: 0.7846 - loss: 0.4740 - val_accuracy: 0.9367 - val_loss: 0.3295 - learning_rate: 0.0010\n",
      "Epoch 12/20\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 826us/step - accuracy: 0.8045 - loss: 0.4627 - val_accuracy: 0.9467 - val_loss: 0.2919 - learning_rate: 0.0010\n",
      "Epoch 13/20\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 830us/step - accuracy: 0.8142 - loss: 0.4258 - val_accuracy: 0.9433 - val_loss: 0.2630 - learning_rate: 0.0010\n",
      "Epoch 14/20\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8019 - loss: 0.4324 - val_accuracy: 0.9467 - val_loss: 0.2375 - learning_rate: 0.0010\n",
      "Epoch 15/20\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 873us/step - accuracy: 0.8375 - loss: 0.3708 - val_accuracy: 0.9533 - val_loss: 0.2276 - learning_rate: 0.0010\n",
      "Epoch 16/20\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 820us/step - accuracy: 0.8733 - loss: 0.3192 - val_accuracy: 0.9533 - val_loss: 0.1981 - learning_rate: 0.0010\n",
      "Epoch 17/20\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 884us/step - accuracy: 0.8928 - loss: 0.2831 - val_accuracy: 0.9600 - val_loss: 0.1919 - learning_rate: 0.0010\n",
      "Epoch 18/20\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 883us/step - accuracy: 0.8647 - loss: 0.3066 - val_accuracy: 0.9667 - val_loss: 0.1619 - learning_rate: 0.0010\n",
      "Epoch 19/20\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 880us/step - accuracy: 0.9167 - loss: 0.2713 - val_accuracy: 0.9600 - val_loss: 0.1627 - learning_rate: 0.0010\n",
      "Epoch 20/20\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 799us/step - accuracy: 0.8909 - loss: 0.2790 - val_accuracy: 0.9567 - val_loss: 0.1576 - learning_rate: 0.0010\n",
      "Shape of X: (20, 25)\n",
      "Shape of y: (20,)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Accuracy: 1.0\n",
      "Precision: 1.0\n",
      "Recall: 1.0\n",
      "F1 Score: 1.0\n",
      "Optimal Team Found: ['Kyle Lowry', 'Kyrie Irving', 'Kyrylo Fesenko', 'LaMarcus Aldridge', 'Lamar Odom']\n"
     ]
    }
   ],
   "source": [
    "# Prepare training data\n",
    "X_train, y_train = simulate_data(training_data)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.3, random_state=42)\n",
    "\n",
    "# Compute class weights to handle class imbalance\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "# Build the model\n",
    "model = Sequential()\n",
    "model.add(Input(shape=(X_train.shape[1],)))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Learning rate scheduler\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=0.001)\n",
    "\n",
    "# Train the model with validation data and class weights\n",
    "model.fit(X_train, y_train, epochs=20, batch_size=16, validation_data=(X_val, y_val), class_weight=class_weights, callbacks=[reduce_lr])\n",
    "\n",
    "# Use data in pool for testing\n",
    "X_test, y_test = simulate_data(pool)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, np.round(y_pred))\n",
    "precision = precision_score(y_test, np.round(y_pred))\n",
    "recall = recall_score(y_test, np.round(y_pred))\n",
    "f1 = f1_score(y_test, np.round(y_pred))\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "\n",
    "# Find the optimal team with the highest prediction score\n",
    "best_team_index = np.argmax(y_pred)\n",
    "best_team = pool[best_team_index*5:(best_team_index+1)*5]\n",
    "\n",
    "# Print the optimal team\n",
    "print(f\"Optimal Team Found: {[player[1] for player in best_team]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explain your architecture and how the basketball player characteristics are used as inputs:\n",
    "1. Data Preparation:\n",
    "\n",
    "- Pool and Training Data:\n",
    "    - `pool`: 100 unique players (2019-2022) for testing.\n",
    "    - `training_data`: Up to 5000 players (2019-2022) for model training.\n",
    "\n",
    "2. Feature Extraction:\n",
    "\n",
    "- `extract_features` function creates a dictionary of features for each player:\n",
    "    - True Shooting Percentage (ts_pct)\n",
    "    - Rebounds (reb)\n",
    "    - Defensive Rebound Percentage (dreb_pct)\n",
    "    - Net Rating (rating)\n",
    "    - Assists (ast)\n",
    "\n",
    "3. Simulating Data for Training:\n",
    "\n",
    "- `simulate_data` function generates training data with labels indicating successful teams based on pre-defined weights.\n",
    "\n",
    "4. Model Building and Training:\n",
    "\n",
    "- Converts simulated teams into a NumPy array (X_train).\n",
    "- Splits data into training and validation sets.\n",
    "- Sequential Neural Network with Keras:\n",
    "    - Two hidden layers with ReLU activation.\n",
    "    - Output layer with sigmoid activation for binary classification.\n",
    "- Model trained for 10 epochs with a batch size of 16.\n",
    "\n",
    "5. Evaluation:\n",
    "\n",
    "- Model predictions on testing set (`X_test`).\n",
    "- Evaluation metrics: accuracy, precision, recall, and F1-score.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
